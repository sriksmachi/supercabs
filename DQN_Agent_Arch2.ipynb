{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import math\r\n",
    "from collections import deque\r\n",
    "import collections\r\n",
    "import pickle\r\n",
    "\r\n",
    "# for building DQN model\r\n",
    "from keras import layers\r\n",
    "from keras import Sequential\r\n",
    "from keras.layers import Dense, Activation, Flatten\r\n",
    "from keras.optimizers import Adam\r\n",
    "\r\n",
    "# for plotting graphs\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# Import the environment\r\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\r\n",
    "Time_matrix = np.load(\"TM.npy\")\r\n",
    "env = CabDriver()\r\n",
    "epsiode_length = 24*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state-action and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\r\n",
    "    \r\n",
    "    def __init__(self, state_size, action_size, discount_factor=0.95, learning_rate=0.01,\r\n",
    "                       epsilon=1, epsilon_decay=0.99, epsilon_min=0.01):\r\n",
    "        # Define size of state and action\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size = action_size + 1 # +1 for the no_idle\r\n",
    "        self.discount_factor = discount_factor\r\n",
    "        self.epsilon = 1.0\r\n",
    "        self.learning_rate = learning_rate      \r\n",
    "        self.epsilon_max = epsilon\r\n",
    "        self.epsilon_decay = epsilon_decay \r\n",
    "        self.epsilon_min = epsilon_min\r\n",
    "        self.batch_size = 32     \r\n",
    "        self.state_encoded = 36 # (m + t + d)\r\n",
    "        # create replay memory using deque\r\n",
    "        self.memory = deque(maxlen=2000)\r\n",
    "        # create main model and target model\r\n",
    "        self.model = self.build_model()\r\n",
    "\r\n",
    "\r\n",
    "    # approximate Q function using Neural Network\r\n",
    "    def build_model(self):\r\n",
    "        model = Sequential()\r\n",
    "        model.add(Dense(32, input_dim=self.state_encoded, activation='relu', kernel_initializer='he_uniform'))\r\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\r\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\r\n",
    "        # model.summary()\r\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\r\n",
    "        return model\r\n",
    "\r\n",
    "\r\n",
    "    def get_action(self, state):\r\n",
    "        \"\"\"\r\n",
    "        get action in a state according to an epsilon-greedy approach\r\n",
    "        \"\"\"\r\n",
    "        indexes, actions = env.requests(state)\r\n",
    "        # print(f'indexes: {indexes}, actions : {actions}')\r\n",
    "        if np.random.rand() <= self.epsilon:\r\n",
    "            # get possible requests/actions given a state\r\n",
    "            # choose an action randomly, this will contain the no-ride action\r\n",
    "            return random.choice(actions)\r\n",
    "        else:\r\n",
    "            # get q_value for all actions = this should be 21 (20 actions + no ride)\r\n",
    "            state_encoded = env.state_encod_arch2(state)\r\n",
    "            state_encoded = np.reshape(state_encoded, [1, self.state_encoded])\r\n",
    "            q_value = self.model.predict(state_encoded)\r\n",
    "            # print(f'q_value..{q_value}')\r\n",
    "            # get q_values for the actions/requests which the cab driver received\r\n",
    "            # the index should include the index for no-ride\r\n",
    "            filtered_q_values = [q_value[0][i] for i in [indexes]]\r\n",
    "            # print(f'filtered_q_value..{filtered_q_values}')\r\n",
    "            # get the index with max q-value\r\n",
    "            max_q_value_index = np.argmax(filtered_q_values)\r\n",
    "            # print(f'max_q_value_index..{max_q_value_index}')\r\n",
    "            return actions[max_q_value_index]\r\n",
    "        \r\n",
    "\r\n",
    "    def append_sample(self, state, action, reward, next_state, hours_of_trip):\r\n",
    "        # append the tuple (s, a, r, s', done) to memory (replay buffer) after every action\r\n",
    "        self.memory.append((state, action, reward, next_state, hours_of_trip))\r\n",
    "\r\n",
    "        # Decay in Îµ after we generate each sample from the environment\r\n",
    "        if self.epsilon > self.epsilon_min:\r\n",
    "            self.epsilon *= self.epsilon_decay\r\n",
    "    \r\n",
    "\r\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\r\n",
    "    def train_model(self):\r\n",
    "\r\n",
    "        if len(self.memory) > self.batch_size:\r\n",
    "            # Sample batch from the memory\r\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "            update_output = np.zeros((self.batch_size, self.state_encoded))\r\n",
    "            update_input = np.zeros((self.batch_size, self.state_encoded))\r\n",
    "            action, rewards, trip_hours_per_exp = [], [], []\r\n",
    "            \r\n",
    "            for i in range(self.batch_size):\r\n",
    "                state, action, reward, next_state, hours_of_trip = mini_batch[i]\r\n",
    "                state_encod = env.state_encod_arch2(state)\r\n",
    "                next_state_encod = env.state_encod_arch2(next_state)\r\n",
    "                update_input[i] = state_encod\r\n",
    "                update_output[i] = next_state_encod\r\n",
    "                rewards.append(reward)\r\n",
    "                trip_hours_per_exp.append(hours_of_trip)\r\n",
    "                \r\n",
    "            # Write your code from here\r\n",
    "            # 1. Predict the target from earlier model\r\n",
    "            target = self.model.predict(update_input)   \r\n",
    "                \r\n",
    "            # 2. Get the target for the Q-network\r\n",
    "            target_qval = self.model.predict(update_output)\r\n",
    "\r\n",
    "            for i in range(self.batch_size):\r\n",
    "                # find if it is the terminal state\r\n",
    "                if (trip_hours_per_exp[i] >= epsiode_length):\r\n",
    "                    # if it is the terminal state (i.e. if it is the end of the month) then the target q value is just the reward\r\n",
    "                    target[i] = rewards[i]\r\n",
    "                else:\r\n",
    "                    # if it is not terminal state then the target value = reward + discount * (max(q(next_state, a)))\r\n",
    "                    target[i] = rewards[i] + self.discount_factor * (np.amax(target_qval[i]))\r\n",
    "\r\n",
    "            #3. Update your 'update_output' and 'update_input' batch. Be careful to use the encoded state-action pair\r\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\r\n",
    "                \r\n",
    "                \r\n",
    "        # 4. Fit your model and track the loss values\r\n",
    "\r\n",
    "    def save(self, name):\r\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 1)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "# Testing code. \r\n",
    "state_size = len(env.state_space)\r\n",
    "action_size = len(env.action_space) + 1 # for no-ride\r\n",
    "agent = DQNAgent(state_size, action_size)\r\n",
    "state = env.state_init\r\n",
    "print(state)\r\n",
    "action = agent.get_action(state)\r\n",
    "print(action)\r\n",
    "state, action, reward, next_state, trip_hours = env.step(state, action, Time_matrix, 0)\r\n",
    "# print(state, action, reward, next_state, trip_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 10\r\n",
    "state_size = len(env.state_space)\r\n",
    "action_size = len(env.action_space) + 1 # for no-ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x000001CF87405910>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp9qg411c2.py, line 48)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x000001CF87405910>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmp9qg411c2.py, line 48)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Episode: 0 Total Reward: -71, epsilon: 0.3024044356690215\n",
      "Episode: 1 Total Reward: -232, epsilon: 0.24983705645845267\n",
      "Episode: 2 Total Reward: -73, epsilon: 0.19046145976502743\n",
      "Episode: 3 Total Reward: -279, epsilon: 0.20230002712877712\n",
      "Episode: 4 Total Reward: -154, epsilon: 0.3024044356690215\n",
      "Episode: 5 Total Reward: -114, epsilon: 0.18112695312597027\n",
      "Episode: 6 Total Reward: -158, epsilon: 0.23759255478829303\n",
      "Episode: 7 Total Reward: 24, epsilon: 0.24486529903492946\n",
      "Episode: 8 Total Reward: -311, epsilon: 0.232864462948006\n",
      "Episode: 9 Total Reward: -158, epsilon: 0.2170448966728076\n"
     ]
    }
   ],
   "source": [
    "for episode in range(Episodes):\r\n",
    "\r\n",
    "    # Call the environment\r\n",
    "    env = CabDriver()\r\n",
    "\r\n",
    "    # Call all the initialised variables of the environment\r\n",
    "    state_size = len(env.state_space)\r\n",
    "    action_size = len(env.action_space) + 1 # for no-ride\r\n",
    "    initial_state = env.state_init\r\n",
    "    current_state = env.state_init\r\n",
    "    #Call the DQN agent \r\n",
    "    agent = DQNAgent(state_size, action_size)\r\n",
    "    steps = 0 # cab driver starts with 0 trip hours\r\n",
    "    total_reward = 0\r\n",
    "    \r\n",
    "    while steps <= 24 * 30: # each episode is 30 days long\r\n",
    "        \r\n",
    "        # print(f'starting state...{steps}')\r\n",
    "        \r\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\r\n",
    "        action = agent.get_action(current_state)\r\n",
    "\r\n",
    "        # 2. Evaluate your reward and next state\r\n",
    "        state, action, reward, next_state, hours_of_trip = env.step(current_state, action, Time_matrix, steps)\r\n",
    "        # print(state, action, reward, next_state, hours_of_trip)\r\n",
    "\r\n",
    "        # 3. Append the experience to the memory\r\n",
    "        agent.append_sample(current_state, action, reward, next_state, hours_of_trip)\r\n",
    "        \r\n",
    "        # 4. Train the model by calling function agent.train_model\r\n",
    "        agent.train_model()\r\n",
    "\r\n",
    "        # Numer of days completed = steps, the episode ends when 30 days are completed\r\n",
    "        steps += hours_of_trip \r\n",
    "\r\n",
    "        current_state = next_state\r\n",
    "\r\n",
    "        # 5. Keep a track of rewards, Q-values, loss\r\n",
    "        total_reward += reward\r\n",
    "\r\n",
    "    #save the model \r\n",
    "    if episode % 50 == 0:\r\n",
    "        agent.model.save_weights(\"./models/supercabs.h5\")\r\n",
    "\r\n",
    "    print(f'Episode: {episode} Total Reward: {total_reward}, epsilon: {agent.epsilon}')\r\n",
    "        \r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}