{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import math\r\n",
    "from collections import deque\r\n",
    "import collections\r\n",
    "import pickle\r\n",
    "\r\n",
    "# for building DQN model\r\n",
    "from keras import layers\r\n",
    "from keras import Sequential\r\n",
    "from keras.layers import Dense, Activation, Flatten\r\n",
    "from keras.optimizers import Adam\r\n",
    "\r\n",
    "# for plotting graphs\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# Import the environment\r\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\r\n",
    "Time_matrix = np.load(\"TM.npy\")\r\n",
    "env = CabDriver()\r\n",
    "epsiode_length = 24*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_tracked = []\r\n",
    "loss_tracked = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state-action and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\r\n",
    "    \r\n",
    "    def __init__(self, state_size, action_size, discount_factor=0.95, learning_rate=0.001,\r\n",
    "                       epsilon=1, epsilon_decay=0.0005, epsilon_min=0.01):\r\n",
    "        # Define size of state and action\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size = action_size + 1 # +1 for the no_idle\r\n",
    "        self.discount_factor = discount_factor\r\n",
    "        self.epsilon = epsilon\r\n",
    "        self.learning_rate = learning_rate      \r\n",
    "        self.epsilon_max = epsilon\r\n",
    "        self.epsilon_decay = epsilon_decay \r\n",
    "        self.epsilon_min = epsilon_min\r\n",
    "        self.batch_size = 32     \r\n",
    "        self.state_encoded = 36 # (m + t + d)\r\n",
    "        # create replay memory using deque\r\n",
    "        self.memory = deque(maxlen=2000)\r\n",
    "        # create main model and target model\r\n",
    "        self.model = self.build_model()\r\n",
    "\r\n",
    "    # approximate Q function using Neural Network\r\n",
    "    def build_model(self):\r\n",
    "        model = Sequential()\r\n",
    "        model.add(Dense(32, input_dim=self.state_encoded, activation='relu', kernel_initializer='he_uniform'))\r\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\r\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\r\n",
    "        model.summary\r\n",
    "        model.compile(loss='mse',optimizer=Adam(learning_rate=self.learning_rate))\r\n",
    "        return model\r\n",
    "\r\n",
    "\r\n",
    "    def get_action(self, state):\r\n",
    "        \"\"\"\r\n",
    "        get action in a state according to an epsilon-greedy approach\r\n",
    "        \"\"\"\r\n",
    "        indexes, _ = env.requests(state)\r\n",
    "        # print(f'indexes: {indexes}, actions : {actions}')\r\n",
    "        if np.random.rand() <= self.epsilon:\r\n",
    "            # get possible requests/actions given a state\r\n",
    "            # choose an action randomly, this will contain the no-ride action\r\n",
    "            return random.choice(indexes)\r\n",
    "        else:\r\n",
    "            # get q_value for all actions = this should be 21 (20 actions + no ride)\r\n",
    "            state_encoded = env.state_encod_arch2(state)\r\n",
    "            state_encoded = np.reshape(state_encoded, [1, self.state_encoded])\r\n",
    "            q_value = self.model.predict(state_encoded)\r\n",
    "            # print(f'q_value..{q_value}')\r\n",
    "            # get q_values for the actions/requests which the cab driver received\r\n",
    "            # the index should include the index for no-ride\r\n",
    "            filtered_q_values = [q_value[0][i] for i in indexes]\r\n",
    "            # print(f'filtered_q_value..{filtered_q_values}')\r\n",
    "            # get the index with max q-value\r\n",
    "            max_q_value_index = np.argmax(filtered_q_values)\r\n",
    "            # print(f'max_q_value_index..{max_q_value_index}')\r\n",
    "            return indexes[max_q_value_index]\r\n",
    "        \r\n",
    "\r\n",
    "    def append_sample(self, state, action, reward, next_state, hours_of_trip):\r\n",
    "        # append the tuple (s, a, r, s', done) to memory (replay buffer) after every action\r\n",
    "        self.memory.append((state, action, reward, next_state, hours_of_trip))\r\n",
    "\r\n",
    "\r\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\r\n",
    "    def train_model(self):\r\n",
    "\r\n",
    "        if len(self.memory) > self.batch_size:\r\n",
    "            # Sample batch from the memory\r\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "            update_output = np.zeros((self.batch_size, self.state_encoded))\r\n",
    "            update_input = np.zeros((self.batch_size, self.state_encoded))\r\n",
    "            actions, rewards, trip_hours_per_exp = [], [], []\r\n",
    "            \r\n",
    "            for i in range(self.batch_size):\r\n",
    "                state, action, reward, next_state, hours_of_trip = mini_batch[i]\r\n",
    "                state_encod = env.state_encod_arch2(state)\r\n",
    "                next_state_encod = env.state_encod_arch2(next_state)\r\n",
    "                update_input[i] = state_encod\r\n",
    "                update_output[i] = next_state_encod\r\n",
    "                actions.append(action)\r\n",
    "                rewards.append(reward)\r\n",
    "                trip_hours_per_exp.append(hours_of_trip)\r\n",
    "                \r\n",
    "            # Write your code from here\r\n",
    "            # 1. Predict the target from earlier model\r\n",
    "            target = self.model.predict(update_input)   \r\n",
    "                \r\n",
    "            # 2. Get the target for the Q-network\r\n",
    "            target_qval = self.model.predict(update_output)\r\n",
    "\r\n",
    "            #3. Update your 'update_output' and 'update_input' batch. Be careful to use the encoded state-action pair\r\n",
    "            for i in range(self.batch_size):\r\n",
    "                # find if it is the terminal state\r\n",
    "                if (trip_hours_per_exp[i] >= epsiode_length):\r\n",
    "                    # if it is the terminal state (i.e. if it is the end of the month) then the target q value is just the reward\r\n",
    "                    target[i][actions[i]] = rewards[i]\r\n",
    "                else:\r\n",
    "                    # if it is not terminal state then the target value = reward + discount * (max(q(next_state, a)))\r\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * (np.amax(target_qval[i]))\r\n",
    "\r\n",
    "            # 4. Fit your model and track the loss values\r\n",
    "            history = self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\r\n",
    "            return history\r\n",
    "\r\n",
    "    #This function will append latest Q-values of the 4 Q-values which are being tracked for checking convergence\r\n",
    "    def save_tracking_states(self, states_action_to_track, state_action_tracked_results):\r\n",
    "        for idx, state_action in enumerate(states_action_to_track):\r\n",
    "            print(state_action)\r\n",
    "            state_encoded = env.state_encod_arch2(state_action[0])\r\n",
    "            state_encoded = np.reshape(state_encoded, [1, agent.state_encoded])\r\n",
    "            q_val = agent.model.predict(state_encoded)\r\n",
    "            state_action_tracked_results[idx].append(q_val[0][state_action[1]])\r\n",
    "\r\n",
    " \r\n",
    "    def save(self, name):\r\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 23, 4)\n",
      "18\n",
      "[(2, 10, 1), 9]\n",
      "[[2, 5, 6], 1]\n",
      "[[0.12982076], [0.0]]\n",
      "[(2, 10, 1), 9]\n",
      "[[2, 5, 6], 1]\n",
      "[[0.12982076, 0.12982076], [0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Testing code. \r\n",
    "states_action_to_track = [[(2,10,1), 9], [[2,5,6], 1]]\r\n",
    "state_action_tracked_results = [[],[]]\r\n",
    "state_size = len(env.state_space)\r\n",
    "action_size = len(env.action_space)\r\n",
    "agent = DQNAgent(state_size, action_size)\r\n",
    "state = env.state_init\r\n",
    "print(state)\r\n",
    "action = agent.get_action(state)\r\n",
    "print(action)\r\n",
    "state, reward, next_state, trip_hours = env.step(state, env.action_space[action], Time_matrix, 0)\r\n",
    "(state, reward, next_state, trip_hours)\r\n",
    "agent.save_tracking_states(states_action_to_track, state_action_tracked_results)\r\n",
    "print(state_action_tracked_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Parameters\r\n",
    "\r\n",
    "# Epsilon values per episode\r\n",
    "# 100 - 0.05\r\n",
    "# 1000 - 0.005\r\n",
    "# 10000 - 0.0005\r\n",
    "# 20000 - 0.0003\r\n",
    "# 30000 - 0.0002\r\n",
    "\r\n",
    "episodes = 101  # 20k - 30k \r\n",
    "state_size = len(env.state_space)\r\n",
    "action_size = len(env.action_space) + 1 # for no-ride\r\n",
    "threshold = 100\r\n",
    "policy_threshold = 3000\r\n",
    "agent = DQNAgent(state_size, action_size, epsilon_decay=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total Reward: -364, epsilon: 1, Buffer Length: 142, Trip Hours: 726\n",
      "Episode: 1 Total Reward: -259, epsilon: 1.0, Buffer Length: 274, Trip Hours: 725\n",
      "Episode: 2 Total Reward: -421, epsilon: 0.9517171302557069, Buffer Length: 417, Trip Hours: 724\n",
      "Episode: 3 Total Reward: -441, epsilon: 0.9057890438555999, Buffer Length: 549, Trip Hours: 725\n",
      "Episode: 4 Total Reward: -632, epsilon: 0.8621008966608072, Buffer Length: 681, Trip Hours: 726\n",
      "Episode: 5 Total Reward: -461, epsilon: 0.820543445547202, Buffer Length: 795, Trip Hours: 726\n",
      "Episode: 6 Total Reward: -450, epsilon: 0.7810127752406908, Buffer Length: 935, Trip Hours: 721\n",
      "Episode: 7 Total Reward: -392, epsilon: 0.7434100384749007, Buffer Length: 1057, Trip Hours: 733\n",
      "Episode: 8 Total Reward: -284, epsilon: 0.7076412088215263, Buffer Length: 1187, Trip Hours: 723\n",
      "Episode: 9 Total Reward: -266, epsilon: 0.6736168455752829, Buffer Length: 1307, Trip Hours: 725\n",
      "Episode: 10 Total Reward: -137, epsilon: 0.6412518701055556, Buffer Length: 1438, Trip Hours: 726\n",
      "Episode: 11 Total Reward: -227, epsilon: 0.6104653531155071, Buffer Length: 1576, Trip Hours: 723\n",
      "Episode: 12 Total Reward: -375, epsilon: 0.5811803122766818, Buffer Length: 1697, Trip Hours: 726\n",
      "Episode: 13 Total Reward: -113, epsilon: 0.5533235197330861, Buffer Length: 1816, Trip Hours: 722\n",
      "Episode: 14 Total Reward: 39, epsilon: 0.5268253189934059, Buffer Length: 1932, Trip Hours: 724\n",
      "Episode: 15 Total Reward: 62, epsilon: 0.5016194507534953, Buffer Length: 2000, Trip Hours: 723\n",
      "Episode: 16 Total Reward: -122, epsilon: 0.47764288721360454, Buffer Length: 2000, Trip Hours: 725\n",
      "Episode: 17 Total Reward: -323, epsilon: 0.45483567447604933, Buffer Length: 2000, Trip Hours: 724\n",
      "Episode: 18 Total Reward: 36, epsilon: 0.4331407826292394, Buffer Length: 2000, Trip Hours: 721\n",
      "Episode: 19 Total Reward: 9, epsilon: 0.4125039631431931, Buffer Length: 2000, Trip Hours: 724\n",
      "Episode: 20 Total Reward: -48, epsilon: 0.3928736132199562, Buffer Length: 2000, Trip Hours: 722\n",
      "Episode: 21 Total Reward: -1, epsilon: 0.3742006467597279, Buffer Length: 2000, Trip Hours: 721\n",
      "Episode: 22 Total Reward: -117, epsilon: 0.35643837162004377, Buffer Length: 2000, Trip Hours: 722\n",
      "Episode: 23 Total Reward: -55, epsilon: 0.3395423728610988, Buffer Length: 2000, Trip Hours: 723\n",
      "Episode: 24 Total Reward: 69, epsilon: 0.32347040168526264, Buffer Length: 2000, Trip Hours: 726\n",
      "Episode: 25 Total Reward: 235, epsilon: 0.30818226979308, Buffer Length: 2000, Trip Hours: 722\n",
      "Episode: 26 Total Reward: 180, epsilon: 0.29363974889158817, Buffer Length: 2000, Trip Hours: 729\n",
      "Episode: 27 Total Reward: 363, epsilon: 0.27980647510367246, Buffer Length: 2000, Trip Hours: 727\n",
      "Episode: 28 Total Reward: 17, epsilon: 0.2666478580394326, Buffer Length: 2000, Trip Hours: 722\n",
      "Episode: 29 Total Reward: 230, epsilon: 0.2541309943021904, Buffer Length: 2000, Trip Hours: 723\n",
      "Episode: 30 Total Reward: 14, epsilon: 0.24222458521285967, Buffer Length: 2000, Trip Hours: 725\n",
      "Episode: 31 Total Reward: 237, epsilon: 0.23089885854694553, Buffer Length: 2000, Trip Hours: 729\n",
      "Episode: 32 Total Reward: 121, epsilon: 0.22012549408847562, Buffer Length: 2000, Trip Hours: 722\n",
      "Episode: 33 Total Reward: 115, epsilon: 0.20987755281470882, Buffer Length: 2000, Trip Hours: 724\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\r\n",
    "\r\n",
    "    # Call the environment\r\n",
    "    env = CabDriver()\r\n",
    "\r\n",
    "    # Call all the initialised variables of the environment\r\n",
    "    state_size = len(env.state_space)\r\n",
    "    action_size = len(env.action_space) + 1 # for no-ride\r\n",
    "    initial_state = env.state_init\r\n",
    "    current_state = env.state_init\r\n",
    "    steps = 0 # cab driver starts with 0 trip hours\r\n",
    "    total_reward = 0\r\n",
    "    \r\n",
    "    while steps <= 24 * 30: # each episode is 30 days long\r\n",
    "        \r\n",
    "        # print(f'starting state...{steps}')\r\n",
    "        \r\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\r\n",
    "        action = agent.get_action(current_state)\r\n",
    "\r\n",
    "        # 2. Evaluate your reward and next state\r\n",
    "        # print(f'action..{action}')\r\n",
    "        state, reward, next_state, hours_of_trip = env.step(current_state, env.action_space[action], Time_matrix, steps)\r\n",
    "        # print(state, action, reward, next_state, hours_of_trip)\r\n",
    "\r\n",
    "        # 3. Append the experience to the memory\r\n",
    "        agent.append_sample(current_state, action, reward, next_state, hours_of_trip)\r\n",
    "        \r\n",
    "        # no need to train for every experience we can train based on threshold.\r\n",
    "\r\n",
    "        # 4. Train the model by calling function agent.train_model\r\n",
    "        history = agent.train_model()\r\n",
    "\r\n",
    "        # Numer of days completed = steps, the episode ends when 30 days are completed\r\n",
    "        steps += hours_of_trip \r\n",
    "\r\n",
    "        current_state = next_state\r\n",
    "\r\n",
    "        # 5. Keep a track of rewards, Q-values, loss\r\n",
    "        total_reward += reward\r\n",
    "\r\n",
    "    # TRACKING Q-VALUES\r\n",
    "    if (episode == threshold-1):        \r\n",
    "        agent.initialize_tracking_states(states_track)\r\n",
    "\r\n",
    "    # Save States Tracked  \r\n",
    "    if ((episode+1) % threshold) == 0:   \r\n",
    "        agent.save_tracking_states(states_track)\r\n",
    "        save_obj(agent.states_tracked,'States_tracked')   \r\n",
    "    \r\n",
    "    # Track rewards\r\n",
    "    rewards_tracked.append(total_reward)\r\n",
    "\r\n",
    "    # Track Loss\r\n",
    "    loss_tracked.append(history.history[\"loss\"])\r\n",
    "\r\n",
    "    # Save the model \r\n",
    "    if episode % 50 == 0:\r\n",
    "        agent.model.save_weights(\"./models/supercabs.h5\")\r\n",
    "    \r\n",
    "    print(f'Episode: {episode} Total Reward: {total_reward}, epsilon: {agent.epsilon}, Buffer Length: {len(agent.memory)}, Trip Hours: {steps}')\r\n",
    "\r\n",
    "    agent.epsilon = agent.epsilon_min + (agent.epsilon_max - agent.epsilon_min) * np.exp(-agent.epsilon_decay * episode)\r\n",
    "\r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking Q Value convergence for the states tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes = 30000\r\n",
    "time = np.arange(0,episodes)\r\n",
    "epsilon = []\r\n",
    "for i in range(0,episodes):\r\n",
    "    epsilon.append(0.001 + (1.0 - 0.001) * np.exp(-0.0002*i))\r\n",
    "plt.plot(time, epsilon)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "# Epsilon values per episode\r\n",
    "# 100 - 0.05\r\n",
    "# 1000 - 0.005\r\n",
    "# 10000 - 0.0005\r\n",
    "# 20000 - 0.0003\r\n",
    "# 30000 - 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}